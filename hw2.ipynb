{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 2** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Classification**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Divya** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 23/4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Tobias Alldén, 19950222-4158, allden@student.chalmers.se; Joakim Milleson, 19940210-6497, joamill@student.chalmers.se** <br />"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 1) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 0),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''?\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'')\n",
    "\n",
    "## [Extending Naive Bayes, 4 points]\n",
    "\n",
    "Consider now, the following vector of attributes:\n",
    "\n",
    "* $x_1 = 1$ if customer is younger than 20 and 0 otherwise.\n",
    "* $x_2 = 1$ if customer is between 20 and 30 in age, and 0 otherwise.\n",
    "* $x_3 = 1$ if customer is older than 30 and 0 otherwise\n",
    "* $x_4 = 1$ if customer walks to work and 0 otherwise.\n",
    "\n",
    "Each vector of attributes has a label ''rich'' or ''poor''. Point out potential difficulties with your approach above to training using naive Bayes. Suggest and describe how to extend your naive Bayes method to this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to the theoretical problems\n",
    "## Naive Bayes Classifier\n",
    "To run the naive bayes classifier we first make some assumptions. These are \n",
    "* The probability of a point being in a specific class is the same for all classes (1/K(the number of classes))\n",
    "* The three attributes for all data points are independant of each other, i.e. the probability of being rich is not dependant on being married\n",
    "\n",
    "The general formula for the naive bayes classifier (i.e. a point being in a specific class) is as follows: \n",
    "\n",
    "$p(t_{new} = k | x_{new},X,t) = \\frac{p(x_{new} | t_{new} = k,X,t)*p(t_{new} = k)}{\\sum_{j} p(x_{new} | t_{new} = j,X,t)*p(t_{new} =j)} $\n",
    "\n",
    "We by the assumptions assume that $p(t_{new} = k)$ and $p(t_{new} = j)$ are the same: $\\frac{1}{K}$ = $\\frac{1}{2}$\n",
    "\n",
    "So for a given new point we want to calculate $p(x_{new} | t_{new} = k,X,t)$. We do this by calculating the probability of each atribute being either 0 or 1, depending on the task, I.e. the probability of the first attribute being 1 equals the number of data points in the training data in the given class having the first parameter set to 1, divided by the total training data in that class. \n",
    "\n",
    "We also want to calculate $\\sum_{j} p(x_{new} | t_{new} = j,X,t)*p(t_{new} =j)$ we do this by calculating the probability of the point being in the first class, plus the probability of the point being in the second class (by the computations mentioned above) and multiplying with the probabilities of being in the two classes. \n",
    "\n",
    "### 1 \n",
    "We wish to calculate $p(t_{new} = 1 | (0,1,1), X,t)$. So by the computations mentioned above we get:\n",
    "\n",
    "$p((0,1,1) | t_{new} = 1,X,t)$ = $\\frac{1}{4} * \\frac{2}{4} * \\frac{3}{4} = \\frac{1}{8}$\n",
    "\n",
    "$\\sum_{j} p((0,1,1) | t_{new} = j,X,t)*p(t_{new} =j)$ = $((\\frac{3}{4} * \\frac{1}{4} * \\frac{1}{4} = \\frac{3}{64}) + \\frac{1}{8}) * \\frac{1}{2} = \\frac{11}{128}$\n",
    "\n",
    "We then use bayes classifier to get:\n",
    "$p(t_{new} = 1 | (0,1,1), X,t) = \\frac{\\frac{1}{8} * \\frac{1}{2}}{\\frac{11}{128}} = \\frac{128}{176} = \\frac{8}{11}$\n",
    "\n",
    "### 2\n",
    "We are here missing a data point, we could assume that the probability of the person being healthy is $\\frac{1}{2}$, but that does not reflect the training data very well. As such, we choose to ignore the final attribute and calculate the probability for the person being content given \"not rich\" and \"married\". By the same computations as in subtask 1 we get. \n",
    "\n",
    "$p(t_{new} = 1 | (0,1), X,t) = \\frac{\\frac{3}{8} * \\frac{1}{2}}{\\frac{7}{32}} = \\frac{96}{112} = \\frac{6}{7}$\n",
    "\n",
    "As such we get a higher probability for the person being content, but that is somewhat self-explanatory given that we ignore one potential aspect of contenteness. \n",
    "\n",
    "## Extending Naive Bayes\n",
    "The problems we notice are that we assume that the parameters are independant in our naive bayes approach, here they clearly are not as a person can only belong to one age group at a time, i.e. having a one in one of the three first attributes means having a zero in the others. Further, we also assume that the probability of being in one of the two classes are equal, which is not the case here as only a small percentage of the population are considered rich (assuming \"rich\" is referring to money). Further, the method does in itself (as it does not explain the context) take into account that people might be unemployed (thus not being able to walk to work), or studying. \n",
    "\n",
    "As such, to extend our approach, we need to calculate $p(x_{new} | t_{new} = k,X,t)$ by only calculating the percentage of persons belonging to one age group at a time. So instead of running the naive bayes approach on the entire dataset, we split it into three different experiments, based on the age group and run one experiment per age group, i.e. one experiment could have the labels rich/poor, and the attributes (under 20?, walk to work?). Then we somwhat compensate for the dependance of the age groups. \n",
    "\n",
    "Further we could instead of having the probability for the label = 0.5 using a official source for the percentage of the population being rich.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "## [Bayes classifier, 5 points]\n",
    "\n",
    "Dowload the dataset **\"dataset2.txt\"**. You can use the following code for example:\n",
    "```python\n",
    "from numpy import genfromtxt\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "```\n",
    "The dataset contains $3$-dimensional data, $X$, generated from $2$ classes with labels, $y$ either $+1$ or $-1$.  Each row of $X$ and $y$ contain one observation and one label respectively.  There are $1000$ instances of each class. \n",
    "\n",
    "a. Assume that the class conditional density is spherical Gaussian, and both classes have equal prior. Write the expression for the Bayes (<span style=\"color:red\"> not **naive Bayes**</span>) classifier i.e. derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "It is useful to note that the dependence on training data $X, y$ for class $1$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new} = 1, X, y) = P(x_{new} |\n",
    "\\hat{\\mu}_{1}, \\hat{\\sigma}^{2}_{1})\n",
    "$$\n",
    "\n",
    "where $\\hat{\\mu}_{1} \\in \\mathbb{R}^3$ and $\\hat{\\sigma}^{2}_{1}\\in \\mathbb{R}$ are MLE estimates for mean (3-dimensional) and variance based on training data with label $+1$ (and similarly for class 2 with label $-1$). \n",
    "\n",
    "b. Implement a function **sph_bayes()** which computes the probability of a new test point *Xtest* coming from class $1$ ($P1$) and class $2$ ($P2$). Finally, assign a label *Ytest* to the test point based on the probabilities $P1$ and $P2$.\n",
    "\n",
    "```python\n",
    "def sph_bayes(Xtest, ...): # other parameters needed.\n",
    "\n",
    "    return [P1, P2, Ytest]\n",
    "```\n",
    "c. Write a function **new_classifier()**\n",
    "\n",
    "```python\n",
    "def new_classifier(Xtest, mu1, mu2)\n",
    "    \n",
    "    return [Ytest]\n",
    "```\n",
    "which implements the following classifier,\n",
    "$$\n",
    "f(x) = \\mbox{sign}\\left(\\frac{(\\mu_1 - \\mu_2)^\\top (x - b) }{\\|\\mu_1 -  \\mu_2\\|_2} \\right)\n",
    "$$\n",
    "with $b = \\frac{1}{2}(\\mu_1 + \\mu_2)$.\n",
    "\n",
    "d. Report 5-fold cross validation error for both classifiers.\n",
    "\n",
    "## [DIGITS dataset classifer, 5 points]\n",
    "\n",
    "Load the DIGITS dataset:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "```\n",
    "This dataset contains $1797$ samples of ten handwritten digit classes. You can further query and visualize the dataset using the various attributes of the returned dictionary:\n",
    "```python\n",
    "data = digits.data\n",
    "print(data.shape)\n",
    "target_names = digits.target_names\n",
    "print (target_names)\n",
    "import matplotlib.pyplot as plt\n",
    "y = digits.target\n",
    "plt.matshow(digits.images[0])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "a. Use **new_classifier()** designed previously to do binary classification between classes representing digits \"*5*\" and \"*8*\".\n",
    "\n",
    "b. Investigate an alternative feature function as described below:\n",
    "\n",
    "1. Scale each pixel value to range $[0, 1] $ from original gray-scale ($0-255$). \n",
    "2. Compute variance of each row and column of the image. This will give you a new feature vector of size $16$ i.e. \n",
    "\n",
    "$$ \n",
    "x' = \\left[ \\; Var(row_1)  , Var(row_2), \\ldots , Var(row_{8}), Var(col_1), \\ldots, Var(col_{8}) \\;\\right]^T\n",
    "$$\n",
    "\n",
    "c. Report $5$-fold cross validation results for parts $(a)$ and\n",
    "$(b)$ in a single table. What can you say about the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to the practical problems\n",
    "## Bayes classifier\n",
    "### a) \n",
    "We want to write the expression for the bayes classifier on this training data set. The general form of the classifiers are as follows: \n",
    "\n",
    "$p(T_{new}=k|X,t,x_{new}) = \\frac{p(x_{new}|T_{new}=k,X,t)p(T_{new} = k,X,t)}{\\sum_{j}p(x_{new}|t_{new}=j,X,t)p(T_{new}=j,X,t)}$\n",
    "\n",
    "We want to select the prior: $p(T_{new} = k,X,t)$ and the likleyhood: $p(x_{new}|T_{new}=k,X,t)$. \n",
    "\n",
    "We will select a class sized prior: $p(T_{new} = k,X,t) = \\frac{N_C}{N}$, i.e. the number of objects in the training data belonging to class/label C divided by the total number of points in the training data. For the likelihood we are given that $p(x_{new}|T_{new}=k,X,t) = p(x_{new}|\\hat{\\mu_c},\\hat{\\sigma^2_c})$ i.e. $E[X]$ in the distribution for the element corresponding to that class from the training data.\n",
    "\n",
    "Thus, for the two classes we get:\n",
    "\n",
    "$p(T_{new}=1|X,t,x_{new}) = \\frac{p(x_{new}|\\hat{\\mu_1},\\hat{\\sigma^2_1})*\\frac{N_1}{N}}{\\sum_j p(x_{new}|\\hat{\\mu_j}\\hat{\\sigma^2_j})*\\frac{N_j}{N}}$\n",
    "\n",
    "$p(T_{new}=-1|X,t,x_{new}) = \\frac{p(x_{new}|\\hat{\\mu_{-1}},\\hat{\\sigma^2_{-1}})*\\frac{N_-{1}}{N}}{\\sum_j p(x_{new}|\\hat{\\mu_j}\\hat{\\sigma^2_j})*\\frac{N_j}{N}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Classifier Error percentage:  100.0 % SphBayes error percentage:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "import scipy.stats as scp\n",
    "from sklearn.model_selection import KFold\n",
    "data = genfromtxt('dataset2.txt', delimiter=',')\n",
    "labels = data[:,-1]\n",
    "posData = data[:1000,:3]\n",
    "negData = data[1000:2000,:3]\n",
    "\n",
    "\n",
    "#b) Sph_bayes classifier as calculated in a)\n",
    "def sph_bayes(Xtest,mu1, mu2, sigma1, sigma2, class1Elements, class2Elements): \n",
    "    dataSize = class1Elements *2\n",
    "    dist1 = scp.multivariate_normal(mu1,sigma1)\n",
    "    likelihood1 = dist1.pdf(Xtest)\n",
    "    dist2 = scp.multivariate_normal(mu2,sigma2)\n",
    "    likelihood2 = dist2.pdf(Xtest)\n",
    "    \n",
    "    prior1 = (class1Elements/dataSize)\n",
    "    prior2 = (class2Elements/dataSize)\n",
    "    \n",
    "    P1 = (likelihood1*prior1)/((likelihood1*prior1)+(likelihood2*prior2))\n",
    "    P2 = (likelihood2*prior2)/((likelihood1*prior1)+(likelihood2*prior2))\n",
    "    \n",
    "    Ytest = -1\n",
    "    \n",
    "    if(P1>P2):\n",
    "        Ytest = 1\n",
    "    return [P1, P2, Ytest]\n",
    "\n",
    "\n",
    "\n",
    "#c) New classifier given to us\n",
    "def new_classifier(Xtest, mu1, mu2):\n",
    "    b = np.divide((np.add(mu1, mu2)),2)\n",
    "    normalized = np.linalg.norm(mu1-mu2)\n",
    "    Ytest = np.sign((mu1-mu2).T.dot(Xtest-b)/normalized)\n",
    "    return [Ytest]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#d) Cross validation between the two classifiers, i.e. split the data into 5 equal parts, train on 4 and validate on one, do this for all permutations of the splits\n",
    "def crossValidation(posData, negData, folds=5):\n",
    "    kf = KFold(folds, shuffle=True)\n",
    "    splittedData = kf.split(posData)\n",
    "    \n",
    "    errorsNewClassifier = 0\n",
    "    errorsSphBayes = 0\n",
    "    \n",
    "    for train_index, test_index in splittedData:\n",
    "        XPosTrain, XPosTest = posData[train_index], posData[test_index]\n",
    "        XNegTrain, XNegTest = negData[train_index], negData[test_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        mu1 = np.mean(XPosTrain, axis=0)\n",
    "        mu2 = np.mean(XNegTrain,axis=0)\n",
    "        sigma1 = np.cov(XPosTrain,rowvar=False)\n",
    "        sigma2 = np.cov(XNegTrain,rowvar=False)\n",
    "        \n",
    "        for xVal in XPosTrain: \n",
    "            _, _, lsp1 = sph_bayes(xVal, mu1, mu2, sigma1, sigma2, XPosTrain.size, XNegTrain.size)\n",
    "            lnc1 = new_classifier(xVal, mu1, mu2)\n",
    "            if lsp1 != 1: \n",
    "                errorsSphBayes+=1\n",
    "            \n",
    "            if lnc1 != 1:\n",
    "                errorsNewClassifier+=1\n",
    "                \n",
    "                \n",
    "        for xVal in XNegTrain: \n",
    "            _, _, lsp1 = sph_bayes(xVal, mu1, mu2, sigma1, sigma2, XPosTrain.size, XNegTrain.size)\n",
    "            lnc1 = new_classifier(xVal, mu1, mu2)\n",
    "            \n",
    "            if lsp1 != -1: \n",
    "                errorsSphBayes+=1\n",
    "            \n",
    "            if lnc1 != -1:\n",
    "                errorsNewClassifier+=1\n",
    "                \n",
    "    nmbOfTests = posData.shape[0] * (folds-1) * 2     \n",
    "    return errorsNewClassifier / nmbOfTests, errorsSphBayes / nmbOfTests\n",
    "    \n",
    "       \n",
    "newClassErr, sphBayErr = crossValidation(posData, negData)      \n",
    "print(\"New Classifier Error percentage: \", newClassErr*100, \"%\", \"SphBayes error percentage: \", sphBayErr*100, '%')    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIGITS dataset classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3194: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  **kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:105: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:127: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error percentage for unmodified data:  10.294936004451865 %\n",
      "Error percentage for modified data:  10.127991096271565 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import KFold\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "data = digits.data\n",
    "digits5 =data[digits.target == 5]\n",
    "digits8 = data[digits.target == 8]\n",
    "realData = digits.target[(digits.target == 5) | (digits.target == 8)]\n",
    "\n",
    "# Run the new classifier on digits = 5 and digits=8 as per assignment a)\n",
    "def run_new_classifier(xTest): \n",
    "    mu1 = np.mean(digits5,axis=0)\n",
    "    mu2 = np.mean(digits8,axis=0)\n",
    "    return new_classifier(xTest,mu1,mu2)\n",
    "    \n",
    "# b) New features as described in task \n",
    "def altFeatures(data):\n",
    "    scaledData = data/16\n",
    "    \n",
    "    featureVector = np.empty(shape=(data.shape[0],16))\n",
    "    \n",
    "    for row in range(0,data.shape[0]):\n",
    "        for col in range(0,8):\n",
    "            featureVector[row][col] = np.var(data[row][8*row:(row+1)*8])\n",
    "            featureVector[row][col+8] = np.var(data[row][col::8])\n",
    "    return featureVector\n",
    "\n",
    "# c) Kfold (k=5) cross validation for the two types  of data, either non-scaled or scaled. \n",
    "def kFold(data,realData,nbrOfFolds=5):\n",
    "    folds = KFold(nbrOfFolds, shuffle=True)\n",
    "    foldedData = folds.split(realData)\n",
    "    \n",
    "    errorsUnmodified = 0\n",
    "    errorsModified = 0\n",
    "    \n",
    "    for train_index, test_index in foldedData:\n",
    "        xTrain, xTest = data[train_index], data[test_index]\n",
    "        realTrain, realTest = realData[train_index], realData[test_index]\n",
    "        \n",
    "        xTrainLabel5 = xTrain[realTrain == 5]\n",
    "        xTrainLabel8 = xTrain[realTrain == 8]\n",
    "        muTrain5 = np.mean(xTrainLabel5, axis = 0)\n",
    "        muTrain8 = np.mean(xTrainLabel8, axis = 0)\n",
    "        \n",
    "        xTrainModified = altFeatures(xTrain)\n",
    "        xTestModified = altFeatures(xTest)\n",
    "        xTrainModLabel5 = xTrainModified[realTrain == 5]\n",
    "        xTrainModLabel8 = xTrainModified[realTrain == 8]\n",
    "        \n",
    "        muTrainMod5 = np.mean(xTrainModLabel5, axis=0)\n",
    "        muTrainMod8 = np.mean(xTrainModLabel8, axis=0)\n",
    "\n",
    "        unmodifiedError = 0\n",
    "        modifiedError = 0\n",
    "        for (i,test) in enumerate(xTest):\n",
    "            label = new_classifier(test,muTrain5,muTrain8)\n",
    "            if label[0] > 0:\n",
    "                label = 5\n",
    "            else:\n",
    "                label = 8\n",
    "            if label != realTest[i]:\n",
    "                errorsUnmodified +=1\n",
    "        \n",
    "        \n",
    "        for (i,test) in enumerate(xTestModified):\n",
    "            label = new_classifier(test,muTrainMod5,muTrainMod8)\n",
    "            if label[0] > 0:\n",
    "                label = 5\n",
    "            else:\n",
    "                label[0] = 8\n",
    "            if label != realTest[i]:\n",
    "                errorsModified +=1\n",
    "                \n",
    "    return errorsUnmodified / (data.shape[0]), errorsModified / (data.shape[0])\n",
    "\n",
    "errUnMod , errMod = kFold(data,realData,5)\n",
    "print(\"Error percentage for unmodified data: \", errUnMod*100,'%')\n",
    "print(\"Error percentage for modified data: \", errMod*100,'%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflections\n",
    "In the first practical assignment we get 100% fail rate for the new classifier and 0 % fail rate for sph_bayes classifier, leading us to believe that something has gone wrong somewhere in the code. \n",
    "\n",
    "In the second practical assignment we see that the error percentage for the modified data is somewhat lower than that of the unmodified data. This might be due to the fact that when the data is normalized it loses some decimal numbers. We also get a degree of freedom divide error which we could not seem to figure out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
