{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 231 Machine Learning: Homework 4** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: Support Vector Machines**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Vasileios** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 14/5** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Name, Personal no., email** <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guidelines:\n",
    "* All solutions to theoretical problems, can be submitted as a single file named *report.pdf*. They can also be submitted in this ipynb notebook, but equations wherever required, should be formatted using LaTeX math-mode.\n",
    "* All discussion regarding practical problems, along with solutions and plots should be specified here itself. We will not generate the solutions/plots again by running your code.\n",
    "* Your name, personal number and email address should be specified above and also in your file *report.pdf*.\n",
    "* All datasets can be downloaded from the course website.\n",
    "* All tables and other additional information should be included."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical problems\n",
    "\n",
    "## [SVM, 5 points]\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Plot these six training points, and construct by inspection the\n",
    "weight vector for the optimal hyperplane. In your solution, specify\n",
    "the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =\n",
    "0$. Calculate what the margin is (i.e., $2\\gamma$ where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point), showing all\n",
    "of your work. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.)\n",
    "\n",
    "## [SVM cont'd, 5 points]\n",
    "\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "a. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given.\n",
    "\n",
    "b. Give the optimal primal solution **for this specific**.\n",
    "\n",
    "c. Write the dual formulation **for this specific**.\n",
    "\n",
    "d. Give the optimal dual solution, comment on support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Solutions Theoretical Problems \n",
    "## SVM 1\n",
    "We wish to calculate the optimal hyperplane, i.e. the optimal decision boundary line that separates the two classes. The fact that it becomes a line is that the hyperplane of a 2D-space is a 1D line. If we plot the data (see plot below), we can see that we have 4 support vectors. \n",
    "\n",
    "$(2,0)^T , (0,2)^T : -1 $\n",
    "\n",
    "$(2,2)^T, (4,0)^T : 1$\n",
    "\n",
    "We know that the weight vector is perpendicular to the decision boundry and has the form $W = w_1x_1 + w_2x_2 + b = 0$\n",
    "In order to calculate this we put in the values for the support vectors such that:\n",
    "$2w_1 + 0w_2x_2 + b = -1$\n",
    "$0x_1 + 2x_2 + b = -1$\n",
    "$2x_1 + 2x_2 + b = 1$\n",
    "$4x_1 + 4x_2 + b = 1$\n",
    "\n",
    "Taking these and solving them gives us the parameters \n",
    "\n",
    "$w_1 = 1 ; w_2 = 1 ; b = 3$. To calculate that our hyperplane correctly classifies the given data we know that sign(W^T X + b) $\\leq$ -1 for points in class -1 and sign(W^T X + b) $\\geq$ 1 in points with class 1. Inserting the point $(0,0)$ yields the result $-3$ and inserting $(4,4)$ yields $5$, thus the decision boundary correctly classifies the points. To calculate the margin $2\\lambda$ and we have that we have that $\\lambda = \\frac{1}{||W||} = \\frac{1}{\\sqrt{w_1^2 + w_2^2}}$. And thus have the margin $2*\\frac{1}{\\sqrt{w_1^2 + w_2^2}} = \\frac{2}{\\sqrt{2}}$\n",
    "\n",
    "\n",
    "\n",
    "## SVM 2\n",
    "### a)\n",
    "\n",
    "For the primal the general formulation is the same with additional constraint as the data:\n",
    "\n",
    "$\\underset{w}{argmin} \\frac{1}{2}w^Tw$\n",
    "\n",
    "$s.t.: $\n",
    "\n",
    "$1*(w^T(2,2) + b) \\geq 1$\n",
    "\n",
    "$1*(w^T(4,0) + b) \\geq 1$\n",
    "\n",
    "$-1*(w^T(2,0) + b) \\geq 1$\n",
    "\n",
    "$-1*(w^T(0,2) + b) \\geq 1$\n",
    "\n",
    "This is an optimization problem that can be solved using lagrange multipliers. Extending whith these yields the new optimization problem; $\\underset{wn\\alpha}{{argmin}} \\frac{||w^T||}{2} - \\sum_{n=1}^N\\alpha_n(t_n(w^Tx_n+b)-1)$\n",
    "### b)\n",
    "To find the min argument here we derivate the expression with regards to w and b and get $w=\\sum_{n=1}^N\\alpha_nt_nx_n$. and $\\frac{d}{db} = \\sum_{n=1}^N\\alpha_nt_n$, we are looking for the minimum argument, i.e. when the derivative is equal to 0, if we insert our support vectors $x_i$ in this equation we get the optimal primal of $\\begin{align}\n",
    "    w &= \\begin{bmatrix}\n",
    "           2\\alpha_1+4\\alpha_2-3\\alpha_3 \\\\\n",
    "           2\\alpha_2-2\\alpha_4\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}$\n",
    "  Where we also know that $\\alpha_1+\\alpha_2-\\alpha_3-\\alpha_4 = 0$ and $\\alpha_n \\geq 0$\n",
    "  \n",
    "### c)\n",
    "Inserting back these derivatives into the initial problem yields the expression: \n",
    "$\\underset{w,\\alpha}{argmax}  \\sum_{n=1}^N\\alpha_n - \\frac{1}{2}\\sum_{n,m=1}^N\\alpha_m\\alpha_nt_mt_nx_m^Tx_n$ where we know that $\\sum_{n=1}^N\\alpha_n = 0$ i.e. $\\alpha_1+\\alpha_2-\\alpha_3-\\alpha_4 = 0$ and also that $\\alpha_n \\geq 0$. This expression we need to maximize in order to find the optimal dual\n",
    "\n",
    "### d) \n",
    "In our example we have four support vectors, they each have a lagrange term and a label as mentioned below. \n",
    "$[(2,2)^T,t_n = 1,\\alpha_1],[(4,0)^T,t_n=1,\\alpha_2],[(2,0)^T,t_n=-1,\\alpha_3],[(0,2)^T,t_n=-1,\\alpha_4]$ Iserting this into the equation yields \n",
    "\n",
    "$\\underset{w,\\alpha}{argmax} (\\alpha_1 + \\alpha_2 + \\alpha_3 + \\alpha_4) - (4\\alpha_1\\alpha_1+8\\alpha_1\\alpha_2-4\\alpha_1\\alpha_3-8\\alpha_2\\alpha_3-4\\alpha_1\\alpha_4)$ where $\\sum_{n=1}^N\\alpha_n = 0$ i.e. $\\alpha_1+\\alpha_2-\\alpha_3-\\alpha_4 = 0$ and $\\alpha_n \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical problems\n",
    "\n",
    "All data files can be loaded using the following code for example:\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.loadtxt('d1.txt')\n",
    "X = data[:,:2]\n",
    "Y = data[:,-1]\n",
    "```\n",
    "\n",
    "For this assigment, you may use sklearn.svm.SVC or sklearn.svm.NuSVC. \n",
    "\n",
    "## [SVM, 5 points]\n",
    "\n",
    "Consider the dataset **d1.txt** having two-dimensional points $X$ and corresponding labels $Y$.\n",
    "\n",
    "a. Train an SVM (with linear kernel) on the above data.\n",
    "\n",
    "b. Plot the data and the separating hyperplane of the trained classifier. Color the points of one class red and the other blue. Mark points that could not be separated correctly. Also, mark the support vectors.\n",
    "\n",
    "c. Does the classifier have bias, and if so, what is it?\n",
    "\n",
    "d. What is the (soft) margin?\n",
    "\n",
    "## [Kernels, 5 points]\n",
    "\n",
    "Download the dataset **d2.txt**. Consider the following kernels.\n",
    "\n",
    "* Linear kernel\n",
    "* Quadratic kernel \n",
    "* Radial Basis Function (RBF) kernel\n",
    "\n",
    "a. Train an SVM with the linear kernel on all of the data (d2.txt). Plot the data. Color the points of one class red and the other blue. Mark points that could not be separated correctly by the trained model.\n",
    "\n",
    "b. Plot the decision surfaces for the Quadratic and RBF kernel seperately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
